
# Modèles linéaires

## Modèle gaussien {#sec-mlg}

À ce stade, nous n'avons fait aucune hypothèse statistique ni probabiliste sur le modèle : les $\bx_i, y_i$ étaient donnés tels quels. Le *modèle linéaire gaussien* avec variables explicatives $\bx_1, \dotsc, \bx_n$ exogènes consiste à supposer que $Y = X\bt + \varepsilon$, où $\varepsilon = N(0,\sigma^2 I_n)$. Formellement, le modèle est indexé par $\bt$ et $\sigma^2$, et donné par 
$$P_{\bt, \sigma^2} = N(X\bt, \sigma^2 I_n).$$ 
Dans ce modèle, la loi de l'estimateur @eq-mco est connue. Par simplicité, je note $H = X(X^\top X)^{-1}X^\top$ la matrice de projection orthogonale sur l'espace vectoriel engendré par les colonnes de $X$, qui est de dimension $d$.

:::{#thm-glm}

## Loi de $\hat{\theta}$

Sous le modèle linéaire gaussien $P_{\bt, \sigma^2}$, 
$$\hat{\bt} \sim N(\bt, \sigma^2 (X^\top X)^{-1}), $$
$$\frac{|\hat{\varepsilon}|^2}{\sigma^2} \sim \chi_2(n-d), $$
et ces deux variables aléatoires sont indépendantes. 

:::

:::{.proof} 

Ce n'est rien de plus que le théorème de Cochran appliqué à notre problème : en effet, le vecteur des résidus est la projection orthogonale de $Y$ sur le sous-espace orthogonal à l'espace des colonnes de $X$. 

:::

La variable aléatoire $|\hat\varepsilon|^2$ est souvent appelée *Somme des Carrés des Résidus* (SCR). Le théorème précédent implique que 
$$\hat{\sigma}^2_n = \frac{|\hat\varepsilon|^2}{n-d}$$
est un estimateur sans biais de $\sigma^2$. et ces deux variables aléatoires sont indépendantes. En particulier, $(n-d)\hat{\sigma}^2_n/\sigma^2 \sim \chi_2(n-d)$. 


## Modèle linéaire général {#sec-mlgen}

Il est possible de ne pas faire d'hypothèses gaussiennes sur le modèle. Dans ce cadre plus général, on supposera que $Y = X\bt + \varepsilon$, où les $\varepsilon_i$ sont iid, centrés, et de même variance $\sigma^2$ --- sous cette dernière hypothèse, on parle de modèle *homoscédastique*. 

Sous ces hypothèses, $\hat{\bt}$ est toujours un estimateur sans biais de $\bt$ : cela se voit directement en prenant l'espérance de @eq-mco-alt. De plus, la loi de $\bt$ n'est plus gaussienne, mais $\bt$ est asymptotiquement normal sous des hypothèses supplémentaires sur $X$. Ces hypothèses sont les suivantes. 

:::{#hyp-?}
On suppose que les variables explicatives $\bx_i$ vérifient la propriété suivante^[On rappelle que $\bx_i$ est un vecteur ligne de taille $d$, et donc que les matrices $\bx_i^\top \bx_i$ sont bien des matrices carrées de taille $d \times d$.] : 
$$ \lim_{n \to \infty}\frac{1}{n}\sum_{i=1}^n \bx_i^\top \bx_i = \Sigma_x,$${#eq-hyp-mco-x}
où $\Sigma_x$ est inversible. Cette propriété s'écrit aussi $X^\top X / n \to \Sigma_x$. 

:::


:::{#thm-mco-an}

Sous les hypothèses précédentes, $\sqrt{n}(\hat{\bt} - \bt)$ converge en loi lorsque $n\to\infty$ vers $N(0,\sigma^2\Sigma_x^{-1}).$

:::

:::{.proof}
  
Rappelons que $\hat{\bt}$ peut s'écrire $\bt + (X^\top X/n)^{-1}\frac{1}{n}\sum_{i=1}^n \bx_i^\top \varepsilon_i$. 
Pour montrer que $\sqrt{n}(\hat{\bt}-\bt)$ converge, il suffit donc de démontrer que 
$$\sqrt{n}\frac{1}{n}\sum_{i=1}^n \bx_i^\top \varepsilon_i $${#eq-proof-tcl-mco}
converge en loi vers $N(0,\Sigma_x^2)$ : comme le terme $(X^\top X/n)^{-1}$ converge vers $\Sigma_x^{-1}$ par hypothèse, la limite de $\sqrt{n}(\hat{\bt} - \bt)$ sera bien $N(0,\Sigma_x^{-1}\Sigma_x\Sigma_x^{-1}) = N(0,\Sigma_x^{-1})$. Malheureusement, on ne peut pas directement appliquer le TCL classique à @eq-proof-tcl-mco : en effet, les variables aléatoires $X_i = \bx_i^\top \varepsilon_i$ ne sont pas identiquement distribuées. On doit pour cela appliquer une version plus générale du TCL, que j'ai écrite en appendice (@thm-mann-wald). Pour appliquer ce théorème en toute rigueur, on a besoin d'une hypothèse supplémentaire sur les $\bx_i$ que je n'ai pas mentionnée --- c'est une hypothèse technique^[Il faut que la quantité $\max_{j=1, \dotsc, n}|\bx_j|^2 / \sum |\bx_i|^2$ tende vers zéro lorsque $n\to\infty$. Cela revient à dire que toute l'information apportée par les $x_i$ n'est pas concentrée sur une seule observation ou sur un très petit nombre d'observations.  ].



:::




## Ellipsoïde de confiance

Les deux théorèmes énoncés ci-dessus permettent de définir des régions de confiance associées à $\bt$ ; ici, $\bt$ n'est plus un nombre réel mais un vecteur, d'où le terme de *région* et plus simplement d'*intervalle*. 

### Préliminaire : la variance est connue {.unnumbered}

Commençons par construire une région probable pour un vecteur gaussien $\xi \sim N(0,I_d)$. Nous savons que $|\xi|^2 \sim \chi_2(d)$. Si $\kappa_{d,1-\alpha}$ désigne le quantile d'ordre $1-\alpha$ de cette loi, on en déduit que $\xi$ est de norme inférieure à $\sqrt{\kappa_{d,1-\alpha}}$ avec probabilité $1-\alpha$ ; autrement dit, 
$$ \mathbb{P}(0 \in B(\xi, \sqrt{\kappa})) = 1-\alpha.$$
Il est immédiat d'en déduire que si $\xi \sim N(\mu, I_d)$, alors comme $\xi - \mu \sim N(0,I_d)$, on a
$$ \mathbb{P}(\mu \in B(\xi, \sqrt{\kappa})) = 1-\alpha. $$
Maintenant, en toute généralité, si $\xi \sim N(\mu,\Sigma)$, alors $\Sigma^{-1/2}(\xi - \mu) \sim N(0,I_d)$. On en déduit donc que 
$$ \mathbb{P}(\mu \in \Sigma^{1/2} B(\xi, \sqrt{\kappa})) = 1-\alpha.$$
La région de confiance est donc l'image de la boule $B(\xi, \sqrt{\kappa})$ par la matrice symétrique $\Sigma^{1/2}$ : c'est une ellipse. Par ailleurs, l'ensemble $\Sigma B(x, \delta)$ peut aussi s'écrire $\{y \in \mathbb{R}^d : |\Sigma^{1/2}(x - y)|^2 \leqslant \delta\}$. 

En combinant ce résultat avec la loi de $\hat{\bt}$ donnée dans @thm-glm, on obtient la région de confiance 
$$ \left\lbrace \theta \in \mathbb{R}^d : \left|\frac{1}{\sigma}(X^{\top} X)^{1/2}(\hat{\bt} - \theta) \right|^2 < \kappa_{d,1-\alpha} \right\rbrace.$$
Malheureusement, cette région nécessite de connaître $\sigma$. Lorsqu'on ne le connaît pas, il faut l'estimer. 

### Cas général {.unnumbered}

Toujours sous le modèle linéaire gaussien, nous avons vu que la loi de $|\sigma^{-1}(X^{\top} X)^{1/2}(\hat{\bt} - \bt)|^2$ est une $\chi_2(d)$, et que la loi de $(n-d)\hat{\sigma}_n^2\sigma^{-2}$
est une $\chi_2(n-d)$. Par conséquent, la variable aléatoire 
$$\frac{|(X^\top X)^{1/2}(\hat{\bt} - \bt)|^2}{\hat{\sigma}_n^2}$$
a pour loi le rapport de lois du $\chi_2$ indépendantes de paramètres $d$ et $n-d$. Cette loi est connue : elle est égale à $d$ fois une *loi de Fisher* dont les propriétés sont données dans @sec-fisher. Cela donne directement le théorème suivant. 

:::{#thm-ellipse}

## Ellipsoïde de confiance

Soit $\hat{\bt}$ l'estimateur des MCO dans un modèle linéaire gaussien. 

Si $f_{d,n-d,1-\alpha}$ est le quantile d'ordre $1-\alpha$ d'une loi $\mathscr{F}_{d,n-d}$, alors la région 
$$ \left\lbrace \theta \in \mathbb{R}^d : \frac{|(X^\top X)^{1/2}(\hat{\bt} - \bt) |^2 / d}{\hat{\sigma}_n^2 } <  f_{d,n-d,1-\alpha} \right\rbrace $$
est une région de confiance de niveau $1-\alpha$ pour $\bt$. 

Lorsque le modèle n'est pas gaussien, mais qu'il vérifie les hypothèses de la section @sec-mlgen, le même résultat est valable mais le niveau de confiance de la région ci-dessus est *asymptotiquement* égal à $1-\alpha$. 

:::
